{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pakete laden und Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Daten einlesen\n",
    "df = pd.read_excel(\"bushel_series_DataChallenge_2025-numisdata4-2_TypenEinzeln.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicht benötigte Spalten/Zeilen löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalten Typ_2 bis Typ_7 löschen, falls vorhanden\n",
    "for spalte in [f\"Typ_{i}\" for i in range(2, 8)]:\n",
    "    if spalte in df.columns:\n",
    "        df.drop(columns=spalte, inplace=True)\n",
    "\n",
    "# Spalte 'Type | Reverse design' löschen\n",
    "df.drop(columns=\"Type | Reverse design\", inplace=True)\n",
    "\n",
    "# Spalte 'Type | Obverse design' löschen\n",
    "df.drop(columns=\"Type | Obverse design\", inplace=True)\n",
    "\n",
    "# Zeilen löschen, wenn 'Findspot' leer ist\n",
    "df = df[~(df['Findspot'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spalte Findspot bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verarbeite_findspot(zelle_findspot):\n",
    "    zelle_findspot = str(zelle_findspot).strip()\n",
    "    klammer_findspot = re.findall(r'\\((.*?)\\)', zelle_findspot)\n",
    "    ohne_klammer_findspot = re.sub(r'\\s*\\(.*?\\)', '', zelle_findspot)\n",
    "    teile_findspot = re.split(r'\\s*[|,]\\s*', ohne_klammer_findspot.strip())\n",
    "    teile_findspot = [t for t in teile_findspot if t] + klammer_findspot\n",
    "    return teile_findspot\n",
    "\n",
    "# Im DataFrame anwenden\n",
    "findspot_liste = df['Findspot'].apply(verarbeite_findspot)\n",
    "\n",
    "# Bestimme maximale Anzahl Spalten\n",
    "spalten_findspot = [f'Findspot_{i+1}' for i in range(findspot_liste.map(len).max())]\n",
    "\n",
    "# Liste in Spalten umwandeln und zum DataFrame hinzufügen\n",
    "df[spalten_findspot] = pd.DataFrame(findspot_liste.tolist(), index=df.index)\n",
    "\n",
    "# Originalspalte löschen\n",
    "df.drop(columns='Findspot', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spalte Type_1 bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spalte_typ1=\"Typ_1\"\n",
    "\n",
    "def verarbeite_typ1_zelle(zelle_typ1):\n",
    "    if pd.isna(zelle_typ1) or zelle_typ1.strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    # Aufteilen und Duplikate entfernen\n",
    "    teile_typ1 = [t_typ1.strip() for t_typ1 in zelle_typ1.strip('|').split('|') if t_typ1.strip()]\n",
    "    unique_teile_typ1 = []\n",
    "    gesehen_typ1 = set()\n",
    "    for t_typ1 in teile_typ1:\n",
    "        if t_typ1 not in gesehen_typ1:\n",
    "            gesehen_typ1.add(t_typ1)\n",
    "            unique_teile_typ1.append(t_typ1)\n",
    "\n",
    "    # Separate Speicherung für Bushel series und Silver unit\n",
    "    output_typ1_1_2 = [None, None]  # [Typ_1_1, Typ_1_2]\n",
    "    rest_typ1_1_2 = []\n",
    "\n",
    "    for eintrag_typ1_1_2 in unique_teile_typ1:\n",
    "        if eintrag_typ1_1_2 == \"Bushel series\":\n",
    "            output_typ1_1_2[0] = \"Bushel series\"\n",
    "        elif eintrag_typ1_1_2 == \"Silver unit\":\n",
    "            output_typ1_1_2[1] = \"Silver unit\"\n",
    "        else:\n",
    "            rest_typ1_1_2.append(eintrag_typ1_1_2)\n",
    "\n",
    "    return output_typ1_1_2 + rest_typ1_1_2  # ergibt: [Bushel series?, Silver unit?, rest...]\n",
    "\n",
    "# Liste erzeugen\n",
    "typ1_liste = df[spalte_typ1].apply(verarbeite_typ1_zelle)\n",
    "\n",
    "# Max. Anzahl Elemente\n",
    "max_typ1 = typ1_liste.map(len).max()\n",
    "\n",
    "# Neue Spalten erzeugen\n",
    "spalten_typ1 = [f'{spalte_typ1}_{i+1}' for i in range(max_typ1)]\n",
    "df[spalten_typ1] = pd.DataFrame(typ1_liste.tolist(), index=df.index)\n",
    "\n",
    "# Originalspalte löschen\n",
    "df.drop(columns=spalte_typ1, inplace=True)\n",
    "\n",
    "# Spalte Typ_1_4 bereinigen\n",
    "df[\"Typ_1_4\"] = df[\"Typ_1_4\"].replace(\n",
    "    to_replace=r\"\\b(Nick|Kellner|Ziegaus)\\b\\s*\", \n",
    "    value=\"\", \n",
    "    regex=True\n",
    ").str.strip()\n",
    "\n",
    "# Zeilen löschen, wenn kein Typ_1_4 vorhanden oder 'Uncertain type' ist\n",
    "df = df[~(df['Typ_1_4'].isna() | (df['Typ_1_4'].str.strip() == 'Uncertain type'))]\n",
    "\n",
    "# Kopie von Typ_1_4 erstellen\n",
    "df['Typ'] = df['Typ_1_4'].copy()\n",
    "\n",
    "# Funktion zur Aufteilung und Mapping\n",
    "def verarbeite_typ_zelle(zelle_stamp):\n",
    "    if pd.isna(zelle_stamp):\n",
    "        return pd.Series([None, None])  # [Typ, Stamp]\n",
    "    \n",
    "    teile_stamp = zelle_stamp.strip().split()\n",
    "    \n",
    "    # Wenn 3 Teile (z.B. \"BS C 4\")\n",
    "    if len(teile_stamp) == 3 and teile_stamp[0] == 'BS':\n",
    "        typ_wert = ' '.join(teile_stamp[:2])  # \"BS C\"\n",
    "        stamp_wert = teile_stamp[2]          # \"4\"\n",
    "    else:\n",
    "        typ_wert = zelle_stamp.strip()\n",
    "        stamp_wert = None\n",
    "\n",
    "    # Mapping für spezielle Fälle\n",
    "    mapping = {\n",
    "        'BS 1': 'BS Prototype',\n",
    "        'BS 2': 'BS A',\n",
    "        'BS 3': 'BS B',\n",
    "        'BS 4': 'BS C',\n",
    "        'BS 5': 'BS D',\n",
    "        'BS 6': 'BS E',\n",
    "        'BS 7': 'BS F',\n",
    "        'BS 8': 'BS G',\n",
    "        'BS 9': 'BS H'\n",
    "    }\n",
    "    \n",
    "    # Mapping anwenden, falls exakt passend\n",
    "    typ_wert = mapping.get(typ_wert, typ_wert)\n",
    "    return pd.Series([typ_wert, stamp_wert])\n",
    "\n",
    "# Anwenden der Funktion auf Spalte 'Typ'\n",
    "df[['Typ', 'Stamp']] = df['Typ'].apply(verarbeite_typ_zelle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spalte \"Findspot | Remark public\" einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Excel einlesen (nur benötigte Spalten)\n",
    "df_remark = pd.read_excel(\"export_Numismatic object_30.5.2025-numisdata4.xlsx\", usecols=[\"Id\", \"Findspot | Remark public\"])\n",
    "\n",
    "# Spalte umbenennen\n",
    "df_remark.rename(columns={\"Findspot | Remark public\": \"Remark public\"}, inplace=True)\n",
    "\n",
    "# Merge mit bestehendem DataFrame über die Spalte 'Id'\n",
    "df = df.merge(df_remark, on=\"Id\", how=\"left\")  # 'left', um alle Zeilen aus df zu behalten\n",
    "\n",
    "# Inhalt der neuen Spalte \"Remark public\" bereinigen\n",
    "def bereinige_remark_public(eintrag_remark):\n",
    "    if pd.isna(eintrag_remark):\n",
    "        return eintrag_remark\n",
    "    eintrag_remark = eintrag_remark.strip()  # führende/trailing Leerzeichen entfernen\n",
    "    \n",
    "    # Mapping für die Ersetzungen\n",
    "    ersetzungen_remark = {\n",
    "        \"Cult places\": \"Cult place\",\n",
    "        \"Cult placeBrandopferplatz\": \"Cult place\",\n",
    "        \"Settlment\": \"Settlement\",\n",
    "        \"SettlementViereckschanze\": \"Settlement\",\n",
    "        \"Single find| Settlement\": \"Single find\"\n",
    "    }\n",
    "\n",
    "      \n",
    "    # Ersetze falls im Dictionary vorhanden\n",
    "    return ersetzungen_remark.get(eintrag_remark, eintrag_remark)  # wenn nicht in ersetzungen: bleibt original\n",
    "\n",
    "# Anwenden auf die Spalte\n",
    "df[\"Remark public\"] = df[\"Remark public\"].apply(bereinige_remark_public)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fehlende Werte im df mit NaN ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(pd.notnull(df), \"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuen Datensatz speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bushel_series_DataChallenge_2025-new.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
